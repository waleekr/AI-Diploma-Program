{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 01 Q Learning\n",
        "\n",
        "## ğŸ“š Learning Objectives | Ø£Ù‡Ø¯Ø§Ù Ø§Ù„ØªØ¹Ù„Ù…\n",
        "\n",
        "This notebook demonstrates key concepts through hands-on examples.\n",
        "\n",
        "By completing this notebook, you will:\n",
        "- Understand the core concepts\n",
        "- See practical implementations\n",
        "- Be ready for exercises\n",
        "\n",
        "## ğŸ”— Prerequisites | Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©\n",
        "\n",
        "- âœ… Python 3.8+ installed\n",
        "- âœ… Required libraries (see `requirements.txt`)\n",
        "\n- âœ… **numpy** library: `pip install numpy`\n",
        "- âœ… Basic Python knowledge\n",
        "\n",
        "---\n",
        "\n",
        "## Code Example | Ù…Ø«Ø§Ù„ Ø§Ù„ÙƒÙˆØ¯\n",
        "\n",
        "Run the code below to see the demonstration:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Unit 2 - Example 1: Q-Learning Algorithm\n",
        "Ø§Ù„ÙˆØ­Ø¯Ø© 2 - Ù…Ø«Ø§Ù„ 1: Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ© Q-Learning\n",
        "\n",
        "This example demonstrates:\n",
        "1. Q-learning algorithm\n",
        "2. Q-table initialization\n",
        "3. Q-value updates\n",
        "4. Policy extraction\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"Example 1: Q-Learning Algorithm\")\n",
        "print(\"Ù…Ø«Ø§Ù„ 1: Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ© Q-Learning\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Simple Grid World Environment\n",
        "# Ø¨ÙŠØ¦Ø© Ø´Ø¨ÙƒØ© Ø¨Ø³ÙŠØ·Ø©\n",
        "# States: 0, 1, 2, 3, 4 (4 is goal)\n",
        "# Actions: 0=left, 1=right\n",
        "\n",
        "class SimpleGridWorld:\n",
        "    \"\"\"\n",
        "    Simple grid world for Q-learning demonstration.\n",
        "    Ø´Ø¨ÙƒØ© Ø¨Ø³ÙŠØ·Ø© Ù„Ø¹Ø±Ø¶ Q-learning.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.states = 5\n",
        "        self.actions = 2  # left, right\n",
        "        self.goal_state = 4\n",
        "        \n",
        "    def get_reward(self, state, action, next_state):\n",
        "        \"\"\"Get reward for transition.\"\"\"\n",
        "        if next_state == self.goal_state:\n",
        "            return 10.0  # Goal reward\n",
        "        return -0.1  # Small negative reward for each step\n",
        "    \n",
        "    def get_next_state(self, state, action):\n",
        "        \"\"\"Get next state after action.\"\"\"\n",
        "        if action == 0:  # left\n",
        "            return max(0, state - 1)\n",
        "        else:  # right\n",
        "            return min(self.states - 1, state + 1)\n",
        "\n",
        "# Initialize Q-table\n",
        "# ØªÙ‡ÙŠØ¦Ø© Ø¬Ø¯ÙˆÙ„ Q\n",
        "env = SimpleGridWorld()\n",
        "Q = np.zeros((env.states, env.actions))\n",
        "\n",
        "print(\"\\nInitial Q-table:\")\n",
        "print(\"Ø¬Ø¯ÙˆÙ„ Q Ø§Ù„Ø£ÙˆÙ„ÙŠ:\")\n",
        "print(Q)\n",
        "\n",
        "# Q-Learning parameters\n",
        "# Ù…Ø¹Ø§Ù…Ù„Ø§Øª Q-Learning\n",
        "alpha = 0.1  # Learning rate\n",
        "gamma = 0.9  # Discount factor\n",
        "epsilon = 0.1  # Exploration rate\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Q-Learning Training\")\n",
        "print(\"ØªØ¯Ø±ÙŠØ¨ Q-Learning\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Training loop\n",
        "# Ø­Ù„Ù‚Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨\n",
        "num_episodes = 100\n",
        "\n",
        "for episode in range(num_episodes):\n",
        "    state = 0  # Start state\n",
        "    \n",
        "    while state != env.goal_state:\n",
        "        # Epsilon-greedy action selection\n",
        "        # Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… epsilon-greedy\n",
        "        if np.random.random() < epsilon:\n",
        "            action = np.random.randint(env.actions)  # Explore\n",
        "        else:\n",
        "            action = np.argmax(Q[state])  # Exploit\n",
        "        \n",
        "        # Take action\n",
        "        next_state = env.get_next_state(state, action)\n",
        "        reward = env.get_reward(state, action, next_state)\n",
        "        \n",
        "        # Q-learning update\n",
        "        # ØªØ­Ø¯ÙŠØ« Q-learning\n",
        "        Q[state, action] = Q[state, action] + alpha * (\n",
        "            reward + gamma * np.max(Q[next_state]) - Q[state, action]\n",
        "        )\n",
        "        \n",
        "        state = next_state\n",
        "\n",
        "print(\"\\nTrained Q-table:\")\n",
        "print(\"Ø¬Ø¯ÙˆÙ„ Q Ø§Ù„Ù…Ø¯Ø±Ø¨:\")\n",
        "print(Q)\n",
        "\n",
        "# Extract policy\n",
        "# Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø³ÙŠØ§Ø³Ø©\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Extracted Policy\")\n",
        "print(\"Ø§Ù„Ø³ÙŠØ§Ø³Ø© Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø©\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "policy = {}\n",
        "for state in range(env.states):\n",
        "    best_action = np.argmax(Q[state])\n",
        "    action_name = \"left\" if best_action == 0 else \"right\"\n",
        "    policy[state] = action_name\n",
        "    print(f\"State {state}: {action_name}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Example completed successfully!\")\n",
        "print(\"ØªÙ… Ø¥ÙƒÙ…Ø§Ù„ Ø§Ù„Ù…Ø«Ø§Ù„ Ø¨Ù†Ø¬Ø§Ø­!\")\n",
        "print(\"=\" * 60)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## âœ… Summary | Ø§Ù„Ù…Ù„Ø®Øµ\n",
        "\n",
        "Great job completing this example!\n",
        "\n",
        "**What you learned:**\n",
        "- Core concepts demonstrated in the code\n",
        "- Practical implementation details\n",
        "\n",
        "**Next steps:**\n",
        "- Complete the exercises in `exercises/` folder\n",
        "- Review the quiz materials\n",
        "- Proceed to the next example\n",
        "\n",
        "---\n",
        "\n",
        "**ğŸ’¡ Tip:** If you see errors, make sure:\n",
        "- All libraries are installed: `pip install -r requirements.txt`\n",
        "- You're using Python 3.8+\n",
        "- Cells are executed in order\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
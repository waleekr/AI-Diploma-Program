{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05. AI-based Learning Models | Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ù…Ø¹ØªÙ…Ø¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ\n",
    "\n",
    "## ğŸ“š Prerequisites (What You Need First) | Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©\n",
    "\n",
    "**BEFORE starting this notebook**, you should have completed:\n",
    "- âœ… **ALL previous notebooks** - This combines everything!\n",
    "  - ğŸ““ **Notebook 1: Search Algorithms** - Algorithm thinking, graph concepts\n",
    "  - ğŸ““ **Notebook 2: Knowledge Representation** - Understanding data structures\n",
    "  - ğŸ““ **Notebook 3: Learning under Uncertainty** - Probability, decision-making\n",
    "  - ğŸ““ **Notebook 4: Optimization Techniques** - Finding optimal solutions\n",
    "\n",
    "**Additional Skills Needed**:\n",
    "- âœ… **Basic Statistics**: Mean, standard deviation (understanding data)\n",
    "- âœ… **Basic Linear Algebra**: Understanding vectors (arrays) conceptually\n",
    "- âœ… **Python Classes**: Object-oriented programming\n",
    "\n",
    "**If you haven't completed previous notebooks**, you will struggle with:\n",
    "- Understanding how models learn (needs optimization from Notebook 4)\n",
    "- Understanding probabilities in predictions (needs Notebook 3)\n",
    "- Understanding data structures (needs Notebook 2)\n",
    "- Understanding algorithm concepts (needs Notebook 1)\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”— Where This Notebook Fits | Ù…ÙƒØ§Ù† Ù‡Ø°Ø§ Ø§Ù„Ø¯ÙØªØ±\n",
    "\n",
    "**This is the FINAL notebook** - it combines everything!\n",
    "\n",
    "**Builds on ALL previous notebooks**:\n",
    "- ğŸ““ **Notebook 1: Search Algorithms** - Algorithm design, graph concepts\n",
    "- ğŸ““ **Notebook 2: Knowledge Representation** - Data structures, reasoning\n",
    "- ğŸ““ **Notebook 3: Learning under Uncertainty** - Probability, Bayesian thinking\n",
    "- ğŸ““ **Notebook 4: Optimization Techniques** - Training models = optimization problem\n",
    "\n",
    "**Leads to**: Advanced topics (Deep Learning, NLP, Computer Vision)\n",
    "\n",
    "**Why this order?**\n",
    "1. **Final notebook**: Combines all concepts learned before\n",
    "2. **Machine Learning = Search + Optimization + Probability + Data Structures**\n",
    "3. **Needs everything**: Models need optimization to train, probability to predict, data structures to store information\n",
    "\n",
    "---\n",
    "\n",
    "## The Story: Teaching Machines to Learn | Ø§Ù„Ù‚ØµØ©: ØªØ¹Ù„ÙŠÙ… Ø§Ù„Ø¢Ù„Ø§Øª Ø§Ù„ØªØ¹Ù„Ù…\n",
    "\n",
    "**THE PROBLEM**: We want machines to make predictions or decisions, but we can't program every rule manually!\n",
    "\n",
    "**BEFORE**: We manually write rules for everything - \"if height > 180cm then tall\" - but this doesn't work for complex patterns.\n",
    "**AFTER**: We use machine learning - show examples to the machine and let it learn patterns automatically!\n",
    "\n",
    "---\n",
    "\n",
    "## âŒ THE PROBLEM: Why Machine Learning Matters | Ø§Ù„Ù…Ø´ÙƒÙ„Ø©: Ù„Ù…Ø§Ø°Ø§ ÙŠÙ‡Ù… Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠØŸ\n",
    "\n",
    "**Problem 1**: We need machines to recognize patterns or make predictions\n",
    "- Spam detection: Can't write rules for all spam patterns\n",
    "- Image recognition: Can't describe what a cat looks like in code\n",
    "- Price prediction: Too many factors to code manually\n",
    "\n",
    "**Problem 2**: Without machine learning, we:\n",
    "- Write manual rules (doesn't work for complex patterns)\n",
    "- Can't adapt to new data (rules become outdated)\n",
    "- Can't handle high-dimensional data (too many features)\n",
    "\n",
    "**WHY IT MATTERS**: Manual rules don't work for complex real-world problems - we need machines that learn!\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… THE SOLUTION: Machine Learning Models | Ø§Ù„Ø­Ù„: Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ\n",
    "\n",
    "**Solution**: Use machine learning algorithms that:\n",
    "1. **Learn from data**: See examples and find patterns\n",
    "2. **Generalize**: Make predictions on new, unseen data\n",
    "3. **Adapt**: Improve with more data\n",
    "\n",
    "**HOW IT SOLVES IT**:\n",
    "- âœ… Instead of manual rules â†’ Learned patterns from data\n",
    "- âœ… Instead of rigid code â†’ Flexible models that adapt\n",
    "- âœ… Instead of feature engineering â†’ Models learn features automatically\n",
    "\n",
    "## Learning Objectives | Ø£Ù‡Ø¯Ø§Ù Ø§Ù„ØªØ¹Ù„Ù…\n",
    "1. Understand the PROBLEM of pattern recognition\n",
    "2. Learn the SOLUTION: Machine learning models\n",
    "3. See HOW they solve real-world problems\n",
    "4. Compare BEFORE (manual rules) vs AFTER (learned models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: âŒ BEFORE - Manual Rules (The Problem) | Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø£ÙˆÙ„: Ù‚Ø¨Ù„ - Ø§Ù„Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„ÙŠØ¯ÙˆÙŠØ© (Ø§Ù„Ù…Ø´ÙƒÙ„Ø©)\n",
    "\n",
    "**THE PROBLEM**: Trying to solve problems with manual rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# âŒ THE PROBLEM: Using manual rules\n",
    "# Example: Predicting house prices\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"âŒ BEFORE: The Problem - Manual Rules\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "print(\"Problem: Predict house price based on size\")\n",
    "print()\n",
    "print(\"âŒ APPROACH: Write manual rules\")\n",
    "print(\"   if size < 50: price = 100,000\")\n",
    "print(\"   elif size < 100: price = 200,000\")\n",
    "print(\"   elif size < 150: price = 300,000\")\n",
    "print(\"   else: price = 500,000\")\n",
    "print()\n",
    "print(\"âŒ PROBLEMS:\")\n",
    "print(\"   1. Rules are arbitrary (why 100k? why 200k?)\")\n",
    "print(\"   2. Doesn't account for location, age, condition...\")\n",
    "print(\"   3. Can't adapt to market changes\")\n",
    "print(\"   4. Doesn't learn from real data\")\n",
    "print()\n",
    "print(\"ğŸ¤” WHY THIS IS A PROBLEM:\")\n",
    "print(\"   â€¢ Real-world problems are complex (many factors)\")\n",
    "print(\"   â€¢ Patterns change over time\")\n",
    "print(\"   â€¢ Can't write rules for everything\")\n",
    "print()\n",
    "print(\"ğŸ’¡ THE SOLUTION: Use machine learning - let the model learn from data!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: âœ… AFTER - Machine Learning (The Solution) | Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø«Ø§Ù†ÙŠ: Ø¨Ø¹Ø¯ - Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ (Ø§Ù„Ø­Ù„)\n",
    "\n",
    "**THE SOLUTION**: Use machine learning to learn patterns from data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… THE SOLUTION: Machine Learning\n",
    "# Show the model examples, let it learn the pattern!\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"âœ… AFTER: The Solution - Machine Learning\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "# Generate example data (simulating real house prices)\n",
    "# Real relationship: price â‰ˆ 2 * size + 1 (with some noise)\n",
    "print(\"   â° WHEN to use: Create synthetic data for learning - simulate real-world scenarios!\")\n",
    "print(\"   ğŸ’¡ WHY use: Synthetic data lets us control the relationship - we know the answer, so we can verify learning works!\")\n",
    "\n",
    "np.random.seed(42)  # Set random seed: Makes results reproducible (same random numbers each time)\n",
    "X = np.random.rand(100, 1) * 10  # House sizes: Generate 100 random house sizes between 0 and 10 (1 feature, 100 samples)\n",
    "y = 2 * X.flatten() + 1 + np.random.randn(100) * 2  # Prices with noise: Calculate price as 2*size + 1, then add random noise (flatten() converts to 1D, randn() adds Gaussian noise)\n",
    "\n",
    "print(\"ğŸ“Š Step 1: Show examples to the model (training data)\")\n",
    "print(f\"   â€¢ {len(X)} examples of (size, price) pairs\")  # Show data size: Display number of training examples\n",
    "print(f\"   â€¢ Model will learn: price = {2:.1f} Ã— size + {1:.1f} (approximately)\")  # Show expected relationship: Display what model should learn (true relationship hidden in data)\n",
    "print()\n",
    "\n",
    "# Split data: some for training (learning), some for testing (evaluating)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  # Split data: 80% for training (model learns from this), 20% for testing (model evaluated on this - unseen data!)\n",
    "\n",
    "print(\"ğŸ“Š Step 2: Train the model (let it learn from examples)\")\n",
    "print(\"   â° WHEN to use: AFTER splitting data - train model on training set\")\n",
    "print(\"   ğŸ’¡ WHY use: This is where model LEARNS - finds the relationship between size and price!\")\n",
    "\n",
    "model = LinearRegression()  # Create learning model: Initialize empty linear regression model (no parameters needed - learns everything from data!)\n",
    "model.fit(X_train, y_train)  # Train it: Show training examples to model, let it learn the pattern (this is where learning happens - model adjusts its weights!)\n",
    "\n",
    "print(f\"   âœ… Model learned: price = {model.coef_[0]:.4f} Ã— size + {model.intercept_:.4f}\")  # Show learned relationship: Display coefficients model learned (should be close to 2.0 and 1.0!)\n",
    "print(f\"   (Close to true relationship: price = 2.0 Ã— size + 1.0)\")  # Compare with truth: Show what relationship model should have learned (for verification)\n",
    "print()\n",
    "\n",
    "# Test on new data (data it hasn't seen before)\n",
    "print(\"   â° WHEN to use: AFTER training - predict on new, unseen test data\")\n",
    "print(\"   ğŸ’¡ WHY use: Test if model generalizes - can it predict on data it hasn't seen? (prevents overfitting!)\")\n",
    "\n",
    "y_pred = model.predict(X_test)  # Make predictions: Use trained model to predict prices for test houses (model has never seen these before!)\n",
    "accuracy = r2_score(y_test, y_pred)  # How well it predicts: Calculate RÂ² score - measures how well predictions match actual values (1.0 = perfect, <1 = some error)\n",
    "\n",
    "print(\"ğŸ“Š Step 3: Test the model on new data\")\n",
    "print(f\"   âœ… RÂ² Score: {accuracy:.4f} (1.0 = perfect, closer to 1 is better)\")  # Show accuracy: Display how well model predicts (should be close to 1.0 if it learned well!)\n",
    "print(f\"   âœ… Model learned the pattern successfully!\")  # Success message: Model successfully learned the relationship!\n",
    "print()\n",
    "\n",
    "print(\"ğŸ”— HOW THIS SOLVES THE PROBLEM:\")\n",
    "print(\"   âŒ BEFORE: Manual rules â†’ Arbitrary, doesn't learn from data\")\n",
    "print(\"   âœ… AFTER: Machine learning â†’ Learns from examples, finds real pattern!\")\n",
    "print(\"   â€¢ Instead of guessing rules â†’ Learns from data\")\n",
    "print(\"   â€¢ Instead of rigid â†’ Adapts to real patterns\")\n",
    "print(\"   â€¢ Instead of arbitrary â†’ Based on actual relationships!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize: BEFORE vs AFTER comparison\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# BEFORE: Manual rules (arbitrary)\n",
    "size = np.linspace(0, 10, 100)\n",
    "manual_price = np.where(size < 5, 100, np.where(size < 10, 200, 500))\n",
    "ax1.plot(size, manual_price, 'r-', linewidth=2, label='Manual rules')\n",
    "ax1.scatter(X_test, y_test, alpha=0.6, color='blue', s=30, label='Real data')\n",
    "ax1.set_xlabel('Size')\n",
    "ax1.set_ylabel('Price')\n",
    "ax1.set_title('âŒ BEFORE: Manual Rules\\n(Arbitrary, doesn\\'t match data)', fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# AFTER: Machine learning (learned from data)\n",
    "ax2.scatter(X_test, y_test, alpha=0.6, color='blue', s=30, label='Real data')\n",
    "ax2.plot(X_test, y_pred, 'r-', linewidth=2, label='ML model prediction')\n",
    "ax2.set_xlabel('Size')\n",
    "ax2.set_ylabel('Price')\n",
    "ax2.set_title(f'âœ… AFTER: Machine Learning\\n(Learned from data, RÂ² = {accuracy:.3f})', fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ğŸ“Š BEFORE vs AFTER Comparison:\")\n",
    "print(\"   BEFORE: Manual rules â†’ Arbitrary, doesn't match real data\")\n",
    "print(\"   AFTER: Machine learning â†’ Learned pattern, matches real data!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: K-Nearest Neighbors (KNN) | Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø«Ø§Ù„Ø«: K-Ø§Ù„Ø¬ÙŠØ±Ø§Ù† Ø§Ù„Ø£Ù‚Ø±Ø¨\n",
    "\n",
    "**The Story**: Some problems don't have clear patterns - but similar things are usually close together!\n",
    "\n",
    "**BEFORE**: We try to find a mathematical formula, but the pattern is too complex or non-linear.\n",
    "**AFTER**: KNN uses a simple idea - \"similar things are close\" - to make predictions without complex formulas!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Nearest Neighbors (KNN) Algorithm\n",
    "# BEFORE: Complex patterns are hard to model with formulas\n",
    "# AFTER: KNN uses simple idea - \"similar things are close\" - to predict!\n",
    "\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class KNN:\n",
    "    \"\"\"\n",
    "    K-Nearest Neighbors: Simple but powerful classification algorithm.\n",
    "    \n",
    "    THE PROBLEM: Complex patterns can't be captured by simple formulas.\n",
    "    THE SOLUTION: Use the simple idea - \"similar things are close together\"!\n",
    "    \n",
    "    How it works:\n",
    "    1. Store all training examples\n",
    "    2. For new example, find K nearest neighbors\n",
    "    3. Predict based on majority class of neighbors\n",
    "    \"\"\"\n",
    "    def __init__(self, k=3):\n",
    "        \"\"\"\n",
    "        Initialize KNN classifier.\n",
    "        \n",
    "        Parameters:\n",
    "        - k: Number of neighbors to consider (usually odd: 3, 5, 7)\n",
    "        \"\"\"\n",
    "        self.k = k\n",
    "        self.X_train = None  # Training features\n",
    "        self.y_train = None  # Training labels\n",
    "        print(f\"âœ… Created KNN classifier (k={k})\")\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train the model (just store the data - KNN is \"lazy\" learning!).\n",
    "        \n",
    "        Unlike other algorithms, KNN doesn't \"learn\" a formula.\n",
    "        It just remembers all examples - that's why it's called \"lazy\"!\n",
    "        \"\"\"\n",
    "        self.X_train = np.array(X)  # Store training features\n",
    "        self.y_train = np.array(y)  # Store training labels\n",
    "        print(f\"  âœ… Stored {len(X)} training examples\")\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict class for new examples.\n",
    "        \n",
    "        THE SOLUTION:\n",
    "        1. For each new example, find K nearest neighbors\n",
    "        2. Look at their classes\n",
    "        3. Predict the majority class\n",
    "        \"\"\"\n",
    "        X = np.array(X)  # Convert to numpy array\n",
    "        predictions = []  # Store predictions\n",
    "        \n",
    "        for x in X:\n",
    "            # Calculate distances to all training examples\n",
    "            # Distance = how \"far\" is this example from each training example?\n",
    "            distances = np.sqrt(np.sum((self.X_train - x)**2, axis=1))\n",
    "            \n",
    "            # Find K nearest neighbors (smallest distances)\n",
    "            k_indices = np.argsort(distances)[:self.k]  # Indices of K nearest\n",
    "            k_nearest_labels = self.y_train[k_indices]  # Classes of K nearest\n",
    "            \n",
    "            # Predict majority class (most common class among neighbors)\n",
    "            majority_class = Counter(k_nearest_labels).most_common(1)[0][0]\n",
    "            predictions.append(majority_class)\n",
    "        \n",
    "        return np.array(predictions)\n",
    "\n",
    "# Example: Classifying fruits by size and sweetness\n",
    "print(\"=\" * 70)\n",
    "print(\"âœ… AFTER: K-Nearest Neighbors (KNN)\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "# Generate example data\n",
    "np.random.seed(42)\n",
    "# Apples: small size, medium sweetness\n",
    "apples = np.random.randn(20, 2) * [0.5, 0.3] + [2, 5]\n",
    "# Oranges: medium size, high sweetness\n",
    "oranges = np.random.randn(20, 2) * [0.5, 0.3] + [5, 7]\n",
    "\n",
    "X = np.vstack([apples, oranges])  # All features\n",
    "y = np.array([0]*20 + [1]*20)  # Labels: 0=Apple, 1=Orange\n",
    "\n",
    "print(\"ğŸ“Š Training data:\")\n",
    "print(f\"   â€¢ {len(apples)} apples (class 0)\")\n",
    "print(f\"   â€¢ {len(oranges)} oranges (class 1)\")\n",
    "print(\"   Features: [size, sweetness]\")\n",
    "print()\n",
    "\n",
    "# Train KNN\n",
    "knn = KNN(k=3)\n",
    "knn.fit(X, y)\n",
    "\n",
    "# Test on new examples\n",
    "test_points = np.array([[2.5, 5.5], [5.5, 7.5], [3.5, 6.0]])\n",
    "predictions = knn.predict(test_points)\n",
    "\n",
    "print(\"ğŸ“Š Predictions on new examples:\")\n",
    "for i, (point, pred) in enumerate(zip(test_points, predictions)):\n",
    "    fruit = \"Apple\" if pred == 0 else \"Orange\"\n",
    "    print(f\"   Point {i+1}: {point} â†’ Predicted: {fruit}\")\n",
    "\n",
    "print(\"\\nğŸ”— HOW THIS SOLVES THE PROBLEM:\")\n",
    "print(\"   âŒ BEFORE: Need complex formula for pattern\")\n",
    "print(\"   âœ… AFTER: KNN uses simple idea - 'similar things are close'!\")\n",
    "print(\"   â€¢ No formula needed - just find nearest neighbors\")\n",
    "print(\"   â€¢ Works for complex, non-linear patterns\")\n",
    "print(\"   â€¢ Simple but powerful!\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(apples[:, 0], apples[:, 1], c='red', label='Apples (training)', s=100, alpha=0.6)\n",
    "plt.scatter(oranges[:, 0], oranges[:, 1], c='orange', label='Oranges (training)', s=100, alpha=0.6)\n",
    "plt.scatter(test_points[:, 0], test_points[:, 1], c=['red' if p==0 else 'orange' for p in predictions], \n",
    "            marker='*', s=300, label='Predictions', edgecolors='black', linewidths=2)\n",
    "plt.xlabel('Size')\n",
    "plt.ylabel('Sweetness')\n",
    "plt.title('âœ… AFTER: KNN Classification\\n(Similar things are close together!)', fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Logistic Regression | Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø±Ø§Ø¨Ø¹: Ø§Ù„Ø§Ù†Ø­Ø¯Ø§Ø± Ø§Ù„Ù„ÙˆØ¬Ø³ØªÙŠ\n",
    "\n",
    "### ğŸ“š Prerequisites (What You Need First)\n",
    "- âœ… **Linear Regression** (from Part 2) - Understanding regression models\n",
    "- âœ… **Probability** (from Notebook 3) - Understanding probabilities (0 to 1)\n",
    "- âœ… **Classification concepts** - Understanding class labels (0/1, Yes/No)\n",
    "\n",
    "### ğŸ”— Relationship: What This Builds On\n",
    "This comes **AFTER** Linear Regression - it's for classification problems!\n",
    "- Builds on: Linear Regression (similar structure, different output)\n",
    "- Alternative to: KNN, Decision Trees (different classification approach)\n",
    "- Best for: Binary classification (two classes)\n",
    "\n",
    "### ğŸ“– The Story\n",
    "Linear Regression predicts continuous values (like price), but what if we want to predict categories (like \"spam\" or \"not spam\")?\n",
    "\n",
    "**Before Logistic Regression**: We can't use linear regression for classification - it outputs continuous values.\n",
    "**After Logistic Regression**: We use logistic function to output probabilities (0 to 1), perfect for classification!\n",
    "\n",
    "---\n",
    "\n",
    "### â° WHEN to use Logistic Regression:\n",
    "- Binary classification (2 classes: Yes/No, Spam/Ham, 0/1)\n",
    "- Need probability estimates (not just class labels)\n",
    "- Linearly separable data (decision boundary is a line)\n",
    "- Interpretable model (understand feature importance)\n",
    "\n",
    "### ğŸ’¡ WHY use Logistic Regression:\n",
    "- Outputs probabilities (0 to 1) - know confidence in predictions\n",
    "- Interpretable - coefficients show feature importance\n",
    "- Fast training and prediction\n",
    "- Good baseline for classification problems\n",
    "- Foundation for neural networks!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression: Binary Classification with Probabilities\n",
    "# â° WHEN to use: Binary classification (2 classes) - need probability estimates!\n",
    "# ğŸ’¡ WHY use: Outputs probabilities (0-1) - know confidence, interpretable coefficients!\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"âœ… AFTER: Logistic Regression (Binary Classification)\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "# THE PROBLEM: Binary Classification\n",
    "print(\"ğŸ“Š THE PROBLEM:\")\n",
    "print(\"   â€¢ Linear Regression predicts continuous values (price, temperature)\")\n",
    "print(\"   â€¢ But we need to predict CATEGORIES (spam/not spam, pass/fail)\")\n",
    "print(\"   â€¢ Categories are discrete (0 or 1), not continuous!\")\n",
    "print()\n",
    "print(\"âœ… THE SOLUTION: Logistic Regression\")\n",
    "print(\"   â€¢ Uses 'logistic function' (S-shaped curve) to output probabilities\")\n",
    "print(\"   â€¢ Probability between 0 and 1 (perfect for classification!)\")\n",
    "print(\"   â€¢ Probability > 0.5 â†’ Class 1, Probability < 0.5 â†’ Class 0\")\n",
    "print()\n",
    "\n",
    "# Generate example data: Email spam classification\n",
    "# Features: [number of spam words, email length]\n",
    "# Label: 0 = Not Spam (Ham), 1 = Spam\n",
    "print(\"ğŸ“Š Example: Email Spam Classification\")\n",
    "print(\"   Features: [number_of_spam_words, email_length]\")\n",
    "print(\"   Label: 0 = Not Spam, 1 = Spam\")\n",
    "print()\n",
    "\n",
    "np.random.seed(42)  # Set random seed: Makes results reproducible\n",
    "\n",
    "# Not Spam emails: Few spam words, various lengths\n",
    "# â° WHEN to use: Generate synthetic data - control the relationship for learning!\n",
    "# ğŸ’¡ WHY use: Synthetic data lets us verify model works - we know the true pattern!\n",
    "not_spam = np.random.randn(50, 2) * [0.5, 20] + [0.5, 100]  # Few spam words (mean=0.5), medium length (mean=100)\n",
    "not_spam_labels = np.zeros(50)  # Labels: All 0 (Not Spam)\n",
    "\n",
    "# Spam emails: Many spam words, short length\n",
    "spam = np.random.randn(50, 2) * [0.5, 15] + [3.0, 80]  # Many spam words (mean=3.0), short length (mean=80)\n",
    "spam_labels = np.ones(50)  # Labels: All 1 (Spam)\n",
    "\n",
    "# Combine data\n",
    "X = np.vstack([not_spam, spam])  # Stack arrays: Combine all features (100 examples, 2 features)\n",
    "y = np.hstack([not_spam_labels, spam_labels])  # Stack labels: Combine all labels (100 labels: 50 zeros, 50 ones)\n",
    "\n",
    "print(f\"   Training data: {len(X)} examples\")\n",
    "print(f\"   â€¢ {len(not_spam)} Not Spam emails (class 0)\")\n",
    "print(f\"   â€¢ {len(spam)} Spam emails (class 1)\")\n",
    "print(f\"   Features: [number_of_spam_words, email_length]\")\n",
    "print()\n",
    "\n",
    "# Split data: training and testing\n",
    "# â° WHEN to use: Split data BEFORE training - separate training from testing!\n",
    "# ğŸ’¡ WHY use: Test on unseen data - verify model generalizes (prevents overfitting!)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)  # Split: 70% train, 30% test\n",
    "\n",
    "print(\"ğŸ“Š Step 1: Train Logistic Regression Model\")\n",
    "print(\"   â° WHEN to use: AFTER splitting data - train on training set\")\n",
    "print(\"   ğŸ’¡ WHY use: This is where model LEARNS - finds relationship between features and spam!\")\n",
    "\n",
    "# Create and train Logistic Regression model\n",
    "# â° WHEN to use: Binary classification - predicting one of two classes!\n",
    "# ğŸ’¡ WHY use: Logistic Regression outputs probabilities - know confidence in predictions!\n",
    "model = LogisticRegression(random_state=42, max_iter=1000)  # Create model: Initialize with random seed, max iterations for convergence\n",
    "model.fit(X_train, y_train)  # Train model: Learn relationship between features (spam words, length) and label (spam/not spam)\n",
    "\n",
    "print(f\"   âœ… Model trained!\")\n",
    "print(f\"   Learned coefficients: {model.coef_[0]}\")  # Show coefficients: Display learned weights for each feature\n",
    "print(f\"   Intercept: {model.intercept_[0]:.4f}\")  # Show intercept: Display bias term\n",
    "print(f\"   â€¢ Coefficient for spam_words: {model.coef_[0][0]:.4f} (positive = more spam words â†’ more likely spam)\")\n",
    "print(f\"   â€¢ Coefficient for email_length: {model.coef_[0][1]:.4f} (negative = longer emails â†’ less likely spam)\")\n",
    "print()\n",
    "\n",
    "# Make predictions (both class labels AND probabilities!)\n",
    "# â° WHEN to use: AFTER training - predict on new, unseen test data\n",
    "# ğŸ’¡ WHY use: Test if model generalizes - can it predict on data it hasn't seen?\n",
    "y_pred = model.predict(X_test)  # Predict classes: Get predicted class (0 or 1) for each test example\n",
    "y_pred_proba = model.predict_proba(X_test)  # Predict probabilities: Get probability of each class (probability of 0 and 1)\n",
    "\n",
    "print(\"ğŸ“Š Step 2: Evaluate Model Performance\")\n",
    "accuracy = accuracy_score(y_test, y_pred)  # Calculate accuracy: Percentage of correct predictions (correct / total)\n",
    "print(f\"   âœ… Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")  # Show accuracy: Display how many predictions were correct\n",
    "print()\n",
    "\n",
    "# Show probability predictions (THE KEY FEATURE of Logistic Regression!)\n",
    "print(\"ğŸ“Š Step 3: Probability Predictions (What makes Logistic Regression special!)\")\n",
    "print(\"   Logistic Regression outputs PROBABILITIES, not just labels!\")\n",
    "print()\n",
    "print(\"   Example predictions (first 5 test examples):\")\n",
    "for i in range(min(5, len(X_test))):  # Loop: Show first 5 examples\n",
    "    prob_not_spam = y_pred_proba[i][0]  # Get probability: Probability of class 0 (Not Spam)\n",
    "    prob_spam = y_pred_proba[i][1]  # Get probability: Probability of class 1 (Spam)\n",
    "    predicted_class = y_pred[i]  # Get prediction: Predicted class (0 or 1)\n",
    "    actual_class = y_test[i]  # Get actual: True class (0 or 1)\n",
    "    \n",
    "    print(f\"   Example {i+1}:\")\n",
    "    print(f\"      Features: spam_words={X_test[i][0]:.2f}, length={X_test[i][1]:.1f}\")\n",
    "    print(f\"      Prob(Not Spam) = {prob_not_spam:.4f} ({prob_not_spam*100:.1f}%)\")\n",
    "    print(f\"      Prob(Spam) = {prob_spam:.4f} ({prob_spam*100:.1f}%)\")\n",
    "    print(f\"      Predicted: {'Spam' if predicted_class == 1 else 'Not Spam'} (Actual: {'Spam' if actual_class == 1 else 'Not Spam'})\")\n",
    "    print()\n",
    "\n",
    "print(\"ğŸ”— HOW THIS SOLVES THE PROBLEM:\")\n",
    "print(\"   âŒ BEFORE: Linear Regression â†’ Continuous output (0.3, 0.7, 1.2...) - not suitable for classification!\")\n",
    "print(\"   âœ… AFTER: Logistic Regression â†’ Probability output (0.0 to 1.0) - perfect for classification!\")\n",
    "print(\"   â€¢ Output between 0 and 1 (probability!)\")\n",
    "print(\"   â€¢ Probability > 0.5 â†’ Class 1, < 0.5 â†’ Class 0\")\n",
    "print(\"   â€¢ Know confidence in predictions (probability = confidence!)\")\n",
    "print(\"   â€¢ Interpretable coefficients (understand feature importance!)\")\n",
    "\n",
    "# Visualize: Decision Boundary\n",
    "print()\n",
    "print(\"ğŸ“Š Step 4: Visualize Decision Boundary\")\n",
    "print(\"   â° WHEN to use: Visualize model decision - see how it separates classes!\")\n",
    "print(\"   ğŸ’¡ WHY use: Visual understanding helps verify model learned correctly!\")\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Plot 1: Training data with decision boundary\n",
    "# Create grid for decision boundary\n",
    "h = 0.1  # Step size: Small step for smooth boundary\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1  # X-axis range: Spam words (min to max)\n",
    "y_min, y_max = X[:, 1].min() - 10, X[:, 1].max() + 10  # Y-axis range: Email length (min to max)\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))  # Create grid: Mesh of points to plot boundary\n",
    "\n",
    "# Predict for each point in grid\n",
    "Z = model.predict(np.c_[xx.ravel(), yy.ravel()])  # Predict: Get class for each grid point\n",
    "Z = Z.reshape(xx.shape)  # Reshape: Convert back to grid shape for plotting\n",
    "\n",
    "ax1.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.RdYlGn)  # Fill regions: Color regions by predicted class (red=spam, green=not spam)\n",
    "ax1.scatter(not_spam[:, 0], not_spam[:, 1], c='green', label='Not Spam (training)', s=50, alpha=0.7, edgecolors='black')  # Plot not spam: Green dots for training not spam emails\n",
    "ax1.scatter(spam[:, 0], spam[:, 1], c='red', label='Spam (training)', s=50, alpha=0.7, edgecolors='black')  # Plot spam: Red dots for training spam emails\n",
    "ax1.set_xlabel('Number of Spam Words', fontsize=11)  # X-axis label: Feature 1\n",
    "ax1.set_ylabel('Email Length', fontsize=11)  # Y-axis label: Feature 2\n",
    "ax1.set_title('Training Data with Decision Boundary', fontweight='bold')  # Title: Describe plot\n",
    "ax1.legend()  # Show legend: Display class labels\n",
    "ax1.grid(True, alpha=0.3)  # Add grid: Light gray grid for readability\n",
    "\n",
    "# Plot 2: Test data predictions\n",
    "ax2.scatter(X_test[y_test == 0, 0], X_test[y_test == 0, 1], c='green', label='Not Spam (test)', s=50, alpha=0.7, edgecolors='black')  # Plot not spam test: Green dots\n",
    "ax2.scatter(X_test[y_test == 1, 0], X_test[y_test == 1, 1], c='red', label='Spam (test)', s=50, alpha=0.7, edgecolors='black')  # Plot spam test: Red dots\n",
    "\n",
    "# Highlight correct vs incorrect predictions\n",
    "correct_mask = (y_pred == y_test)  # Find correct: Mask for correct predictions\n",
    "incorrect_mask = ~correct_mask  # Find incorrect: Mask for incorrect predictions\n",
    "\n",
    "if np.any(incorrect_mask):  # Check if any wrong: Only plot if there are incorrect predictions\n",
    "    ax2.scatter(X_test[incorrect_mask, 0], X_test[incorrect_mask, 1], c='yellow', marker='x', s=200, linewidths=3, label='Incorrect', zorder=5)  # Highlight wrong: Yellow X marks incorrect predictions\n",
    "\n",
    "ax2.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.RdYlGn)  # Same boundary: Show decision boundary on test plot\n",
    "ax2.set_xlabel('Number of Spam Words', fontsize=11)  # X-axis label: Feature 1\n",
    "ax2.set_ylabel('Email Length', fontsize=11)  # Y-axis label: Feature 2\n",
    "ax2.set_title(f'Test Data Predictions (Accuracy: {accuracy:.2%})', fontweight='bold')  # Title: Show accuracy\n",
    "ax2.legend()  # Show legend: Display class labels and incorrect markers\n",
    "ax2.grid(True, alpha=0.3)  # Add grid: Light gray grid\n",
    "\n",
    "plt.tight_layout()  # Adjust spacing: Prevent labels from overlapping\n",
    "plt.show()  # Display plot: Show visualization\n",
    "\n",
    "print(\"ğŸ“Š Visualization Explanation:\")\n",
    "print(\"   â€¢ Green region = Predicted 'Not Spam'\")\n",
    "print(\"   â€¢ Red region = Predicted 'Spam'\")\n",
    "print(\"   â€¢ Boundary line = Where probability = 0.5 (decision threshold)\")\n",
    "print(\"   â€¢ Model learned to separate spam from not spam!\")\n",
    "print()\n",
    "print(\"ğŸ”— HOW THIS SOLVES THE PROBLEM:\")\n",
    "print(\"   âŒ BEFORE: Can't use Linear Regression for classification (wrong output type)\")\n",
    "print(\"   âœ… AFTER: Logistic Regression outputs probabilities - perfect for classification!\")\n",
    "print(\"   â€¢ Probabilities between 0 and 1\")\n",
    "print(\"   â€¢ Decision boundary separates classes\")\n",
    "print(\"   â€¢ Interpretable coefficients (understand importance of features)\")\n",
    "print(\"   â€¢ Fast and efficient for binary classification!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network from Scratch\n",
    "# BEFORE: Simple models can't learn complex patterns\n",
    "# AFTER: Neural networks with multiple layers can learn complex patterns!\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class SimpleNeuralNetwork:\n",
    "    \"\"\"\n",
    "    Simple Neural Network: Mimics how the brain works!\n",
    "    \n",
    "    THE PROBLEM: Simple models (like linear regression) can't learn complex patterns.\n",
    "    THE SOLUTION: Neural networks with multiple layers can learn ANY pattern!\n",
    "    \n",
    "    How it works:\n",
    "    1. Input layer: Receives data\n",
    "    2. Hidden layer(s): Process data (learn patterns)\n",
    "    3. Output layer: Makes prediction\n",
    "    4. Each neuron: Takes inputs, applies weights, uses activation function\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        \"\"\"\n",
    "        Initialize neural network.\n",
    "        \n",
    "        Parameters:\n",
    "        - input_size: Number of input features\n",
    "        - hidden_size: Number of neurons in hidden layer\n",
    "        - output_size: Number of output classes\n",
    "        \"\"\"\n",
    "        # Initialize weights randomly (small values)\n",
    "        # Weights determine how neurons are connected\n",
    "        self.W1 = np.random.randn(input_size, hidden_size) * 0.1  # Input to hidden\n",
    "        self.b1 = np.zeros((1, hidden_size))  # Bias for hidden layer\n",
    "        self.W2 = np.random.randn(hidden_size, output_size) * 0.1  # Hidden to output\n",
    "        self.b2 = np.zeros((1, output_size))  # Bias for output layer\n",
    "        \n",
    "        print(f\"âœ… Created Neural Network: {input_size} â†’ {hidden_size} â†’ {output_size}\")\n",
    "        print(f\"   Input layer: {input_size} neurons\")\n",
    "        print(f\"   Hidden layer: {hidden_size} neurons\")\n",
    "        print(f\"   Output layer: {output_size} neurons\")\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        \"\"\"\n",
    "        Sigmoid activation function.\n",
    "        \n",
    "        This function:\n",
    "        - Takes any number\n",
    "        - Outputs value between 0 and 1\n",
    "        - Enables non-linear learning (can learn complex patterns!)\n",
    "        \"\"\"\n",
    "        return 1 / (1 + np.exp(-np.clip(x, -250, 250)))  # Clip to prevent overflow\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward pass: Make prediction.\n",
    "        \n",
    "        Process:\n",
    "        1. Input â†’ Hidden: X @ W1 + b1, then apply sigmoid\n",
    "        2. Hidden â†’ Output: hidden @ W2 + b2, then apply sigmoid\n",
    "        \"\"\"\n",
    "        # Layer 1: Input to Hidden\n",
    "        z1 = np.dot(X, self.W1) + self.b1  # Linear combination\n",
    "        a1 = self.sigmoid(z1)  # Apply activation (non-linearity!)\n",
    "        \n",
    "        # Layer 2: Hidden to Output\n",
    "        z2 = np.dot(a1, self.W2) + self.b2  # Linear combination\n",
    "        a2 = self.sigmoid(z2)  # Final output\n",
    "        \n",
    "        return a2, a1  # Return output and hidden layer (for backpropagation)\n",
    "    \n",
    "    def train(self, X, y, epochs=1000, learning_rate=0.1):\n",
    "        \"\"\"\n",
    "        Train the neural network.\n",
    "        \n",
    "        THE SOLUTION: Use backpropagation to learn!\n",
    "        1. Forward pass: Make prediction\n",
    "        2. Calculate error\n",
    "        3. Backpropagate: Update weights to reduce error\n",
    "        4. Repeat until learned!\n",
    "        \"\"\"\n",
    "        print(f\"\\\\nğŸš€ Training Neural Network...\")\n",
    "        print(f\"   Epochs: {epochs}, Learning rate: {learning_rate}\")\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Forward pass\n",
    "            output, hidden = self.forward(X)\n",
    "            \n",
    "            # Calculate error (how wrong are we?)\n",
    "            error = output - y\n",
    "            \n",
    "            # Backpropagation: Update weights to reduce error\n",
    "            # (This is the learning part!)\n",
    "            \n",
    "            # Output layer error\n",
    "            delta2 = error * output * (1 - output)  # Gradient\n",
    "            \n",
    "            # Hidden layer error\n",
    "            error_hidden = delta2.dot(self.W2.T)\n",
    "            delta1 = error_hidden * hidden * (1 - hidden)  # Gradient\n",
    "            \n",
    "            # Update weights (this is how we learn!)\n",
    "            self.W2 -= learning_rate * hidden.T.dot(delta2)\n",
    "            self.b2 -= learning_rate * np.sum(delta2, axis=0, keepdims=True)\n",
    "            self.W1 -= learning_rate * X.T.dot(delta1)\n",
    "            self.b1 -= learning_rate * np.sum(delta1, axis=0, keepdims=True)\n",
    "            \n",
    "            # Show progress\n",
    "            if epoch % 200 == 0:\n",
    "                loss = np.mean(error**2)  # Mean squared error\n",
    "                print(f\"   Epoch {epoch}: Loss = {loss:.4f}\")\n",
    "        \n",
    "        print(\"âœ… Training complete!\")\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make prediction on new data\"\"\"\n",
    "        output, _ = self.forward(X)\n",
    "        return output\n",
    "\n",
    "# Example: Learning XOR function (non-linear problem!)\n",
    "print(\"=\" * 70)\n",
    "print(\"âœ… AFTER: Neural Networks from Scratch\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "# XOR problem: Simple but non-linear (can't be learned by linear models!)\n",
    "# Input: (0,0) â†’ Output: 0\n",
    "# Input: (0,1) â†’ Output: 1\n",
    "# Input: (1,0) â†’ Output: 1\n",
    "# Input: (1,1) â†’ Output: 0\n",
    "\n",
    "X_xor = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y_xor = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "print(\"ğŸ“Š Problem: Learn XOR function (non-linear!)\")\n",
    "print(\"   Input â†’ Output:\")\n",
    "for x, y in zip(X_xor, y_xor):\n",
    "    print(f\"   {x} â†’ {y[0]}\")\n",
    "\n",
    "print(\"\\\\nâŒ PROBLEM: Linear models can't learn this!\")\n",
    "print(\"   (XOR is non-linear - needs hidden layer!)\")\n",
    "\n",
    "# Create and train neural network\n",
    "nn = SimpleNeuralNetwork(input_size=2, hidden_size=4, output_size=1)\n",
    "nn.train(X_xor, y_xor, epochs=2000, learning_rate=0.5)\n",
    "\n",
    "# Test predictions\n",
    "predictions = nn.predict(X_xor)\n",
    "print(\"\\\\nğŸ“Š Predictions:\")\n",
    "for x, pred, true_val in zip(X_xor, predictions, y_xor):\n",
    "    print(f\"   Input {x} â†’ Predicted: {pred[0]:.4f} (True: {true_val[0]})\")\n",
    "\n",
    "print(\"\\\\nğŸ”— HOW THIS SOLVES THE PROBLEM:\")\n",
    "print(\"   âŒ BEFORE: Simple models can't learn non-linear patterns\")\n",
    "print(\"   âœ… AFTER: Neural networks with hidden layers can learn ANY pattern!\")\n",
    "print(\"   â€¢ Multiple layers enable complex learning\")\n",
    "print(\"   â€¢ Activation functions enable non-linearity\")\n",
    "print(\"   â€¢ Can learn incredibly complex patterns!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Cross-Validation | Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø®Ø§Ù…Ø³: Ø§Ù„ØªØ­Ù‚Ù‚ Ø§Ù„Ù…ØªÙ‚Ø§Ø·Ø¹\n",
    "\n",
    "**The Story**: How do we know if our model will work on new data? We need to test it properly!\n",
    "\n",
    "**BEFORE**: We test on the same data we trained on - this gives overly optimistic results (overfitting).\n",
    "**AFTER**: Cross-validation tests on different data - gives realistic estimate of how well the model will perform!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-Validation: Proper Model Evaluation\n",
    "# BEFORE: Test on training data â†’ Overly optimistic (overfitting)\n",
    "# AFTER: Cross-validation â†’ Realistic performance estimate!\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import make_scorer, mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"âœ… AFTER: Cross-Validation (Proper Model Evaluation)\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "# Generate example data\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 1) * 10\n",
    "y = 2 * X.flatten() + 1 + np.random.randn(100) * 2\n",
    "\n",
    "print(\"ğŸ“Š THE PROBLEM:\")\n",
    "print(\"   â€¢ If we test on training data, model looks perfect (overfitting)\")\n",
    "print(\"   â€¢ But it might fail on new, unseen data!\")\n",
    "print(\"   â€¢ We need realistic performance estimate\")\n",
    "print()\n",
    "\n",
    "print(\"âœ… THE SOLUTION: Cross-Validation\")\n",
    "print(\"   â€¢ Split data into K folds (e.g., 5 folds)\")\n",
    "print(\"   â€¢ Train on 4 folds, test on 1 fold\")\n",
    "print(\"   â€¢ Repeat 5 times (each fold used as test once)\")\n",
    "print(\"   â€¢ Average results â†’ Realistic performance!\")\n",
    "print()\n",
    "\n",
    "# Create model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Cross-validation (K-Fold with K=5)\n",
    "print(\"ğŸ“Š Performing 5-Fold Cross-Validation:\")\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_scores = cross_val_score(model, X, y, cv=kfold, scoring='neg_mean_squared_error')\n",
    "\n",
    "print(f\"   Fold 1: MSE = {-cv_scores[0]:.4f}\")\n",
    "print(f\"   Fold 2: MSE = {-cv_scores[1]:.4f}\")\n",
    "print(f\"   Fold 3: MSE = {-cv_scores[2]:.4f}\")\n",
    "print(f\"   Fold 4: MSE = {-cv_scores[3]:.4f}\")\n",
    "print(f\"   Fold 5: MSE = {-cv_scores[4]:.4f}\")\n",
    "print()\n",
    "print(f\"   âœ… Average MSE: {-cv_scores.mean():.4f} Â± {cv_scores.std():.4f}\")\n",
    "print(f\"   âœ… This is realistic performance on new data!\")\n",
    "\n",
    "print(\"\\\\nğŸ”— HOW THIS SOLVES THE PROBLEM:\")\n",
    "print(\"   âŒ BEFORE: Test on training data â†’ Overly optimistic (overfitting)\")\n",
    "print(\"   âœ… AFTER: Cross-validation â†’ Realistic estimate of performance!\")\n",
    "print(\"   â€¢ Tests on different data each time\")\n",
    "print(\"   â€¢ Average gives reliable estimate\")\n",
    "print(\"   â€¢ Know how model will perform on new data!\")\n",
    "\n",
    "print(\"\\\\nğŸ“Š BEFORE vs AFTER:\")\n",
    "print(\"   BEFORE: Train on all data, test on same data â†’ Perfect score (misleading!)\")\n",
    "print(\"   AFTER: Cross-validation â†’ Realistic score (trustworthy!)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Decision Trees | Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø³Ø§Ø¯Ø³: Ø£Ø´Ø¬Ø§Ø± Ø§Ù„Ù‚Ø±Ø§Ø±\n",
    "\n",
    "**The Story**: Sometimes decisions are simple - just ask yes/no questions!\n",
    "\n",
    "**BEFORE**: Complex models are hard to understand.\n",
    "**AFTER**: Decision trees make decisions by asking simple yes/no questions - easy to understand and visualize!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Trees: Simple Yes/No Questions\n",
    "# BEFORE: Complex models are hard to understand\n",
    "# AFTER: Decision trees ask simple yes/no questions - easy to understand!\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"âœ… AFTER: Decision Trees (Simple Yes/No Questions)\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "# Generate example data: Classify fruits by size and sweetness\n",
    "np.random.seed(42)\n",
    "# Apples: small size, medium sweetness\n",
    "apples = np.random.randn(50, 2) * [0.5, 0.3] + [2, 5]\n",
    "# Oranges: medium size, high sweetness\n",
    "oranges = np.random.randn(50, 2) * [0.5, 0.3] + [5, 7]\n",
    "\n",
    "X = np.vstack([apples, oranges])\n",
    "y = np.array([0]*50 + [1]*50)  # 0=Apple, 1=Orange\n",
    "\n",
    "print(\"ğŸ“Š Problem: Classify fruits (Apple vs Orange)\")\n",
    "print(\"   Features: [size, sweetness]\")\n",
    "print()\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train decision tree\n",
    "dt = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = dt.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"ğŸ“Š Decision Tree Decision Process:\")\n",
    "print(\"   The tree asks simple yes/no questions:\")\n",
    "print(\"   1. Is size > threshold? â†’ Yes/No\")\n",
    "print(\"   2. Is sweetness > threshold? â†’ Yes/No\")\n",
    "print(\"   3. Based on answers, predict class\")\n",
    "print()\n",
    "\n",
    "print(f\"âœ… Model accuracy: {accuracy:.1%}\")\n",
    "print()\n",
    "\n",
    "# Visualize decision tree\n",
    "plt.figure(figsize=(14, 8))\n",
    "plot_tree(dt, filled=True, feature_names=['Size', 'Sweetness'], \n",
    "          class_names=['Apple', 'Orange'], fontsize=10)\n",
    "plt.title('âœ… AFTER: Decision Tree Visualization\\n(Simple yes/no questions make decisions!)', \n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ğŸ”— HOW THIS HELPS IN REAL LIFE:\")\n",
    "print(\"   â€¢ Medical: 'Has fever? â†’ Yes â†’ 'Has cough? â†’ Yes â†’ Likely flu'\")\n",
    "print(\"   â€¢ Business: 'Revenue > threshold? â†’ Yes â†’ 'Profit > threshold? â†’ Expand'\")\n",
    "print(\"   â€¢ Customer Service: Ask questions to route to right department\")\n",
    "print()\n",
    "print(\"ğŸ“Š BEFORE vs AFTER:\")\n",
    "print(\"   BEFORE: Complex model â†’ Hard to understand why decisions made\")\n",
    "print(\"   AFTER: Decision tree â†’ Clear, understandable decision process!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Random Forest | Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø³Ø§Ø¨Ø¹: Ø§Ù„ØºØ§Ø¨Ø© Ø§Ù„Ø¹Ø´ÙˆØ§Ø¦ÙŠØ©\n",
    "\n",
    "**The Story**: One tree might be wrong, but many trees together are usually right!\n",
    "\n",
    "**BEFORE**: Single decision tree might overfit or make mistakes.\n",
    "**AFTER**: Random forest uses MANY trees and votes - more accurate and robust!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest: Many Trees Vote\n",
    "# BEFORE: Single decision tree might overfit or make mistakes\n",
    "# AFTER: Random forest uses MANY trees - more accurate and robust!\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"âœ… AFTER: Random Forest (Many Trees Vote)\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "print(\"ğŸ“Š THE PROBLEM: Single decision tree might:\")\n",
    "print(\"   â€¢ Overfit (memorize training data)\")\n",
    "print(\"   â€¢ Make mistakes on new data\")\n",
    "print(\"   â€¢ Be sensitive to small changes\")\n",
    "print()\n",
    "\n",
    "print(\"âœ… THE SOLUTION: Random Forest\")\n",
    "print(\"   â€¢ Train MANY decision trees (each on different data)\")\n",
    "print(\"   â€¢ Each tree votes on the prediction\")\n",
    "print(\"   â€¢ Final prediction = majority vote\")\n",
    "print(\"   â€¢ More trees = more accurate and robust!\")\n",
    "print()\n",
    "\n",
    "# Generate data\n",
    "np.random.seed(42)\n",
    "X, y = make_classification(n_samples=200, n_features=2, n_redundant=0, \n",
    "                          n_informative=2, n_clusters_per_class=1, random_state=42)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train single decision tree\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "dt.fit(X_train, y_train)\n",
    "dt_pred = dt.predict(X_test)\n",
    "dt_accuracy = accuracy_score(y_test, dt_pred)\n",
    "\n",
    "# Train random forest (many trees)\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)  # 100 trees\n",
    "rf.fit(X_train, y_train)\n",
    "rf_pred = rf.predict(X_test)\n",
    "rf_accuracy = accuracy_score(y_test, rf_pred)\n",
    "\n",
    "print(\"ğŸ“Š Comparison:\")\n",
    "print(f\"   Single Decision Tree Accuracy: {dt_accuracy:.1%}\")\n",
    "print(f\"   Random Forest (100 trees) Accuracy: {rf_accuracy:.1%}\")\n",
    "print()\n",
    "\n",
    "if rf_accuracy > dt_accuracy:\n",
    "    improvement = (rf_accuracy - dt_accuracy) / dt_accuracy * 100\n",
    "    print(f\"   âœ… Random Forest is {improvement:.1f}% more accurate!\")\n",
    "    print(\"   âœ… More trees = better accuracy and more robust!\")\n",
    "else:\n",
    "    print(\"   âœ… Both perform well on this dataset\")\n",
    "\n",
    "print(\"\\nğŸ”— HOW THIS HELPS IN REAL LIFE:\")\n",
    "print(\"   â€¢ Medical Diagnosis: Multiple doctors agree â†’ More confident diagnosis\")\n",
    "print(\"   â€¢ Recommendation Systems: Multiple algorithms vote â†’ Better recommendations\")\n",
    "print(\"   â€¢ Image Recognition: Multiple models agree â†’ More reliable\")\n",
    "print()\n",
    "print(\"ğŸ“Š BEFORE vs AFTER:\")\n",
    "print(\"   BEFORE: Single tree â†’ Might overfit, less accurate\")\n",
    "print(\"   AFTER: Random forest â†’ More accurate, more robust, less overfitting!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Real-World Application: Complete ML Pipeline | Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø«Ø§Ù…Ù†: Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ÙˆØ§Ù‚Ø¹ÙŠ: Ø®Ø· Ø£Ù†Ø§Ø¨ÙŠØ¨ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ Ø§Ù„ÙƒØ§Ù…Ù„\n",
    "\n",
    "**The Story**: Let's build a complete machine learning system from scratch - this is how AI works in real life!\n",
    "\n",
    "**BEFORE**: We've seen individual algorithms, but not how they work together.\n",
    "**AFTER**: We build a complete system that combines everything - data, training, evaluation, and deployment!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real-World Application: Complete ML Pipeline\n",
    "# BEFORE: Individual algorithms, but not how they work together\n",
    "# AFTER: Complete system from data to predictions - this is REAL-LIFE AI!\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"âœ… AFTER: Real-World Application - Complete ML Pipeline\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "print(\"ğŸ“Š REAL-LIFE EXAMPLE: House Price Prediction System\")\n",
    "print(\"   This is what real AI systems do:\")\n",
    "print(\"   1. Get data\")\n",
    "print(\"   2. Prepare data\")\n",
    "print(\"   3. Train model (uses optimization!)\")\n",
    "print(\"   4. Evaluate model\")\n",
    "print(\"   5. Make predictions on new data\")\n",
    "print()\n",
    "\n",
    "# Step 1: Simulate real-world data\n",
    "np.random.seed(42)\n",
    "n_samples = 200\n",
    "\n",
    "# Features: size, bedrooms, age\n",
    "X = np.random.rand(n_samples, 3)\n",
    "X[:, 0] = X[:, 0] * 200 + 50  # Size: 50-250 mÂ²\n",
    "X[:, 1] = (X[:, 1] * 5 + 1).astype(int)  # Bedrooms: 1-5\n",
    "X[:, 2] = X[:, 2] * 30  # Age: 0-30 years\n",
    "\n",
    "# Target: price (depends on features with some noise)\n",
    "# Real relationship: price = 1000 Ã— size + 50000 Ã— bedrooms - 1000 Ã— age + noise\n",
    "y = 1000 * X[:, 0] + 50000 * X[:, 1] - 1000 * X[:, 2] + np.random.randn(n_samples) * 20000\n",
    "\n",
    "print(\"ğŸ“Š Step 1: Data Collection\")\n",
    "print(f\"   âœ… Collected {n_samples} house listings\")\n",
    "print(f\"   Features: [Size (mÂ²), Bedrooms, Age (years)]\")\n",
    "print(f\"   Target: Price (SAR)\")\n",
    "print()\n",
    "\n",
    "# Step 2: Prepare data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"ğŸ“Š Step 2: Data Preparation\")\n",
    "print(f\"   âœ… Split data: {len(X_train)} training, {len(X_test)} testing\")\n",
    "print()\n",
    "\n",
    "# Step 3: Train multiple models (compare them!)\n",
    "print(\"ğŸ“Š Step 3: Model Training (Using Optimization!)\")\n",
    "print(\"   Training multiple models to find best one...\")\n",
    "print()\n",
    "\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"   Training {model_name}...\")\n",
    "    \n",
    "    # Train model (this uses optimization internally!)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Evaluate\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    results[model_name] = {\n",
    "        'model': model,\n",
    "        'mse': mse,\n",
    "        'r2': r2,\n",
    "        'predictions': y_pred\n",
    "    }\n",
    "    \n",
    "    print(f\"      âœ… RÂ² Score: {r2:.3f} (closer to 1 is better)\")\n",
    "    print(f\"      âœ… MSE: {mse:,.0f} (lower is better)\")\n",
    "    print()\n",
    "\n",
    "# Step 4: Compare models\n",
    "print(\"ğŸ“Š Step 4: Model Comparison\")\n",
    "print(f\"   {'Model':<25} {'RÂ² Score':<15} {'MSE':<15}\")\n",
    "print(\"   \" + \"-\" * 55)\n",
    "\n",
    "best_model_name = None\n",
    "best_r2 = -float('inf')\n",
    "\n",
    "for model_name, result in results.items():\n",
    "    print(f\"   {model_name:<25} {result['r2']:<15.3f} {result['mse']:<15,.0f}\")\n",
    "    if result['r2'] > best_r2:\n",
    "        best_r2 = result['r2']\n",
    "        best_model_name = model_name\n",
    "\n",
    "print()\n",
    "print(f\"âœ… Best Model: {best_model_name} (RÂ² = {best_r2:.3f})\")\n",
    "print()\n",
    "\n",
    "# Step 5: Use model for predictions\n",
    "print(\"ğŸ“Š Step 5: Make Predictions on New Data\")\n",
    "print(\"   Example: Predict price for new house:\")\n",
    "new_house = np.array([[150, 3, 5]])  # 150 mÂ², 3 bedrooms, 5 years old\n",
    "predicted_price = results[best_model_name]['model'].predict(new_house)[0]\n",
    "\n",
    "print(f\"   House: 150 mÂ², 3 bedrooms, 5 years old\")\n",
    "print(f\"   âœ… Predicted Price: {predicted_price:,.0f} SAR\")\n",
    "print()\n",
    "\n",
    "print(\"ğŸ”— HOW THIS WORKS IN REAL LIFE:\")\n",
    "print(\"   â€¢ Real Estate: Predict house prices â†’ Help buyers/sellers\")\n",
    "print(\"   â€¢ E-commerce: Predict product demand â†’ Manage inventory\")\n",
    "print(\"   â€¢ Healthcare: Predict patient outcomes â†’ Help doctors\")\n",
    "print(\"   â€¢ Finance: Predict stock prices â†’ Help investors\")\n",
    "print()\n",
    "print(\"ğŸ“Š COMPLETE ML PIPELINE:\")\n",
    "print(\"   1. âœ… Data Collection (real-world data)\")\n",
    "print(\"   2. âœ… Data Preparation (clean and split)\")\n",
    "print(\"   3. âœ… Model Training (uses optimization from Notebook 4!)\")\n",
    "print(\"   4. âœ… Model Evaluation (cross-validation from this notebook!)\")\n",
    "print(\"   5. âœ… Model Selection (choose best model)\")\n",
    "print(\"   6. âœ… Make Predictions (use in real life!)\")\n",
    "print()\n",
    "print(\"ğŸ”— CONNECTIONS TO ALL NOTEBOOKS:\")\n",
    "print(\"   â€¢ Notebook 1: Search algorithms â†’ Model training searches for best parameters\")\n",
    "print(\"   â€¢ Notebook 2: Data structures â†’ Store and organize data\")\n",
    "print(\"   â€¢ Notebook 3: Probability â†’ Model predictions are probabilistic\")\n",
    "print(\"   â€¢ Notebook 4: Optimization â†’ Training = optimization problem!\")\n",
    "print(\"   â€¢ This notebook: Everything comes together in machine learning!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: Problem â†’ Solution Realization | Ø§Ù„Ù…Ù„Ø®Øµ: Ø¥Ø¯Ø±Ø§Ùƒ Ø§Ù„Ù…Ø´ÙƒÙ„Ø© â†’ Ø§Ù„Ø­Ù„\n",
    "\n",
    "### âŒ THE PROBLEM:\n",
    "1. Need machines to recognize patterns or make predictions\n",
    "2. Without ML: Manual rules (arbitrary, doesn't adapt)\n",
    "3. This leads to poor performance on complex problems\n",
    "\n",
    "### âœ… THE SOLUTION:\n",
    "1. **Show examples**: Provide training data\n",
    "2. **Let model learn**: Algorithm finds patterns automatically\n",
    "3. **Test on new data**: Verify it generalizes\n",
    "\n",
    "### ğŸ”— HOW THE SOLUTION SOLVES THE PROBLEM:\n",
    "- âœ… Instead of manual rules â†’ Learned patterns from data\n",
    "- âœ… Instead of rigid â†’ Flexible, adapts to real patterns\n",
    "- âœ… Instead of arbitrary â†’ Based on actual relationships\n",
    "\n",
    "### ğŸ“Š BEFORE vs AFTER:\n",
    "- **BEFORE**: Problem â†’ Manual rules â†’ Poor performance\n",
    "- **AFTER**: Problem â†’ Machine learning â†’ Learns patterns, good performance!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "course2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

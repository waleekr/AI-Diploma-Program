# Test 2: Bias, Justice, and Discrimination
## Ø§Ù…ØªØ­Ø§Ù† 2: Ø§Ù„ØªØ­ÙŠØ² ÙˆØ§Ù„Ø¹Ø¯Ø§Ù„Ø© ÙˆØ§Ù„ØªÙ…ÙŠÙŠØ²

**Time Limit:** 60 minutes | **Marks:** 50 points

---

## Instructions
- Answer all questions
- Provide detailed explanations where required
- Use examples to support your answers

---

## Part 1: Multiple Choice (15 points)

### Question 1 (3 points)
What is the difference between disparate impact and disparate treatment?
- A) No difference
- B) Disparate impact is unintentional, disparate treatment is intentional
- C) Disparate impact is intentional, disparate treatment is unintentional
- D) Both are the same

**Answer:** B

---

### Question 2 (3 points)
What are protected attributes?
- A) All features
- B) Characteristics protected from discrimination (race, gender, etc.)
- C) Model parameters
- D) Data columns

**Answer:** B

---

### Question 3 (3 points)
What is demographic parity also known as?
- A) Equal accuracy
- B) Statistical parity
- C) Equal opportunity
- D) Calibration

**Answer:** B

---

### Question 4 (3 points)
What is the main challenge with fairness metrics?
- A) They are easy to achieve
- B) Different definitions may conflict
- C) They don't exist
- D) They are always consistent

**Answer:** B

---

### Question 5 (3 points)
What is bias mitigation?
- A) Ignoring bias
- B) Techniques to reduce unfairness in AI systems
- C) Only data cleaning
- D) Only model tuning

**Answer:** B

---

## Part 2: Short Answer (20 points)

### Question 6 (10 points)
Explain the difference between pre-processing, in-processing, and post-processing bias mitigation techniques. Provide an example of each.

**Answer:**
- **Pre-processing**: Address bias in training data before model training (e.g., rebalancing dataset, removing biased features)
- **In-processing**: Modify training algorithm to enforce fairness (e.g., adding fairness constraints to loss function)
- **Post-processing**: Adjust model outputs after training (e.g., threshold adjustment for different groups)

---

### Question 7 (10 points)
Describe a real-world case of algorithmic bias. What was the issue, who was affected, and how could it have been prevented?

**Answer:**
Example: COMPAS recidivism prediction system showed racial bias, predicting higher risk for Black defendants. Could be prevented by:
1. Diverse training data
2. Regular bias audits
3. Fairness metrics during development
4. Transparency in methodology
5. Human oversight in decisions

---

## Part 3: Case Study (15 points)

### Question 8 (15 points)
A hiring AI system shows bias against certain groups. Describe how you would:
1. Detect the bias
2. Measure its extent
3. Mitigate it
4. Monitor ongoing fairness

**Answer:**
1. **Detection**: Analyze hiring rates by protected attributes, test model outputs, use fairness toolkits
2. **Measurement**: Calculate demographic parity, equalized odds, calibration metrics
3. **Mitigation**: Rebalance training data, add fairness constraints, adjust decision thresholds
4. **Monitoring**: Regular audits, track metrics over time, stakeholder feedback, continuous improvement

---

## Grading Rubric | Ù…Ø¹Ø§ÙŠÙŠØ± Ø§Ù„ØªÙ‚ÙŠÙŠÙ…

### Multiple Choice (15 points)
- 3 points per question

### Short Answer (20 points)
- Question 6: Explanation (6 points), Examples (4 points)
- Question 7: Case description (5 points), Prevention (5 points)

### Case Study (15 points)
- Detection (4 points), Measurement (4 points), Mitigation (4 points), Monitoring (3 points)

---

**Total: 50 points**

---

**Good luck!** ğŸ€

